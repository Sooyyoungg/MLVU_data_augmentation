{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n",
      "torch.Size([100, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "## Segmentation을 위한 모델 설정\n",
    "fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()    #모델설정\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "stack_seg = []\n",
    "for epoch in range(1):  \n",
    "    \n",
    "    for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data                   # inputs =  Batchsize x 3(RGB) x W x H\n",
    "        seg = fcn(inputs)['out']                # Batchsize x 21 x W x H\n",
    "        seg = torch.argmax(seg,dim=1)           # Batchsize x W x H \n",
    "        #여기까지 하시면 각 Batchsize x W x H 의 각 픽셀에 예측되는 픽셀의 클래스 넘버가 부여\n",
    "        # 0이 배경이고 나머지 1~ 는 물체\n",
    "        print(seg.shape)\n",
    "        stack_seg.append(seg)\n",
    "        \n",
    "        \"\"\"\n",
    "        seg = seg.detach().numpy()        # Numpy 로 바꾸는 작업\n",
    "        저는 결과물을 plt.imshow 로 뽑아 확인할때 numpy로 바꾸는 작업이 필요해서 쓰실일이 있을까봐\n",
    "        적어두었습니다.\n",
    "\n",
    "        이 다음부터 0인 픽셀만 골라서 RGB값으로 조정해 주시면 될 것 같습니다.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "output = torch.stack(stack_seg, dim=0)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output, 'segmented_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "arr = []\n",
    "for i in range(len(output)):\n",
    "    for j in range(len(output[0])):\n",
    "        for m in range(32):\n",
    "            for n in range(32):\n",
    "                if output[i][j][m][n] != 0:\n",
    "                    arr.append([i,j])\n",
    "                    #plt.imshow(output[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [(23, 49), (77, 76), (72, 82), (65, 94), (83, 8), (73, 15)]\n"
     ]
    }
   ],
   "source": [
    "dup = list(set(map(tuple, arr)))\n",
    "print(len(dup),dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
